{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b82465",
   "metadata": {},
   "source": [
    "\n",
    "# ABS NAICS3 → CBSA (Step‑Through Notebook)\n",
    "\n",
    "This notebook bakes in the same parameters as the CLI script, but lets you run the pipeline cell‑by‑cell:\n",
    "- Load ABS county × NAICS3\n",
    "- Reconcile county NAICS3 sums to NAICS 00\n",
    "- Aggregate to CBSA\n",
    "- Filter to “large” CBSAs\n",
    "- Save outputs\n",
    "\n",
    "> Update the **Parameters** cell below if your file paths or thresholds differ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b00acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parameters (edit these) ---\n",
    "ABS_PATH = \"abs_county_naics3.csv\"          # path to your ABS county×NAICS3 CSV\n",
    "XWALK_PATH = \"cbsa_county_crosswalk.csv\"    # county→CBSA crosswalk\n",
    "YEAR = 2022                                  # optional year filter; set to None if not present\n",
    "LARGE_BY = \"firms\"                           # \"firms\" or \"population\" (requires cbsa_pop column)\n",
    "LARGE_THRESHOLD = 20000                      # firms count or population threshold (depending on LARGE_BY)\n",
    "RECON_ATOL = 1.0                             # reconciliation absolute tolerance\n",
    "OUTDIR = \"outputs\"                           # where to write CSV outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a1b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3024a7b9-feb2-4c93-8d60-75c386b836b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zfill_series(s, n):\n",
    "    return s.astype(str).str.extract(r\"(\\d+)\", expand=False).fillna(\"\").str.zfill(n)\n",
    "\n",
    "def normalize_abs_columns(df):\n",
    "    # Standardize column names\n",
    "    rename_map = {}\n",
    "    for want in [\"state\",\"county\",\"naics\",\"naics2022\",\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\",\"year\"]:\n",
    "        for actual in df.columns:\n",
    "            if actual.lower() == want:\n",
    "                rename_map[actual] = want.upper() if want in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"] else want\n",
    "    df = df.rename(columns=rename_map)\n",
    "    # Derive NAICS3\n",
    "    if \"NAICS2022\" in df.columns:\n",
    "        df[\"naics3\"] = df[\"NAICS2022\"].astype(str).str[:3]\n",
    "    elif \"naics\" in df.columns:\n",
    "        df[\"naics3\"] = df[\"naics\"].astype(str).str[:3]\n",
    "    else:\n",
    "        raise ValueError(\"ABS file must have NAICS2022 or naics column\")\n",
    "    # Standardize FIPS\n",
    "    df[\"state\"] = zfill_series(df[\"state\"], 2)\n",
    "    df[\"county\"] = zfill_series(df[\"county\"], 3)\n",
    "    # Convert ABS $1,000s → dollars for PAYANN/RCPPDEMP\n",
    "    for k in [\"PAYANN\",\"RCPPDEMP\"]:\n",
    "        if k in df.columns:\n",
    "            df[k] = pd.to_numeric(df[k], errors=\"coerce\") * 1000\n",
    "    # Cast numerics\n",
    "    for c in [\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"year\" in df.columns:\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def normalize_crosswalk(df):\n",
    "    need = [\"state_fips\",\"county_fips\",\"cbsa_code\",\"cbsa_title\"]\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Crosswalk missing columns: {missing}\")\n",
    "    df[\"state_fips\"]  = zfill_series(df[\"state_fips\"], 2)\n",
    "    df[\"county_fips\"] = zfill_series(df[\"county_fips\"], 3)\n",
    "    if \"cbsa_pop\" in df.columns:\n",
    "        df[\"cbsa_pop\"] = pd.to_numeric(df[\"cbsa_pop\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def reconcile_county_totals(abs_df, year=None, atol=1.0):\n",
    "    base = abs_df.copy()\n",
    "    if year is not None and \"year\" in base.columns:\n",
    "        base = base[base[\"year\"] == year]\n",
    "    mask_all = base[\"naics3\"].isin([\"000\"]) | base.get(\"NAICS2022\",\"\").astype(str).isin([\"00\"]) | base.get(\"naics\",\"\").astype(str).isin([\"00\"])\n",
    "    all_rows = base[mask_all].copy()\n",
    "    all_rows[\"naics3\"] = \"000\"\n",
    "    parts = base[~mask_all].copy()\n",
    "\n",
    "    key = [\"state\",\"county\"]\n",
    "    sums = (parts.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                .sum(min_count=1))\n",
    "    totals = (all_rows.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                    .sum(min_count=1))\n",
    "\n",
    "    # Rename for clarity\n",
    "    sums = sums.rename(columns={\"FIRMPDEMP\":\"sum_firmpdemp\",\"EMP\":\"sum_emp\",\"PAYANN\":\"sum_payann\",\"RCPPDEMP\":\"sum_rcppdemp\"})\n",
    "    totals = totals.rename(columns={\"FIRMPDEMP\":\"tot_firmpdemp\",\"EMP\":\"tot_emp\",\"PAYANN\":\"tot_payann\",\"RCPPDEMP\":\"tot_rcppdemp\"})\n",
    "    rep = sums.merge(totals, on=key, how=\"outer\")\n",
    "\n",
    "    for c in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"]:\n",
    "        rep[f\"delta_{c}\"] = rep[f\"sum_{c}\"] - rep[f\"tot_{c}\"]\n",
    "        rep[f\"pct_delta_{c}\"] = np.where(rep[f\"tot_{c}\"].abs() > 0, rep[f\"delta_{c}\"]/rep[f\"tot_{c}\"], np.nan)\n",
    "        rep[f\"flag_{c}\"] = rep[f\"delta_{c}\"].abs() > atol\n",
    "    rep[\"recon_ok\"] = ~(rep[[f\"flag_{c}\" for c in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"]]].any(axis=1))\n",
    "    return rep.sort_values(key)\n",
    "\n",
    "def aggregate_to_cbsa(abs_df, xwalk_df, year=None):\n",
    "    base = abs_df.copy()\n",
    "    if year is not None and \"year\" in base.columns:\n",
    "        base = base[base[\"year\"] == year]\n",
    "    base[\"state_fips\"] = base[\"state\"]\n",
    "    base[\"county_fips\"] = base[\"county\"]\n",
    "    merged = base.merge(xwalk_df, on=[\"state_fips\",\"county_fips\"], how=\"left\", validate=\"m:1\")\n",
    "    parts = merged[~merged[\"naics3\"].isin([\"000\"])].copy()\n",
    "    key = [\"cbsa_code\",\"cbsa_title\",\"naics3\"]\n",
    "    out = (parts.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                .sum(min_count=1))\n",
    "    all_cbsa = (merged[merged[\"naics3\"].isin([\"000\"])]\n",
    "                .groupby([\"cbsa_code\",\"cbsa_title\"], as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                .sum(min_count=1)\n",
    "                .rename(columns={\n",
    "                    \"FIRMPDEMP\":\"cbsa_tot_firms\",\n",
    "                    \"EMP\":\"cbsa_tot_emp\",\n",
    "                    \"PAYANN\":\"cbsa_tot_payroll\",\n",
    "                    \"RCPPDEMP\":\"cbsa_tot_receipts\"\n",
    "                }))\n",
    "    out = out.merge(all_cbsa, on=[\"cbsa_code\",\"cbsa_title\"], how=\"left\")\n",
    "    return out\n",
    "\n",
    "def filter_large_cbsa(cbsa_df, xwalk_df, large_by=\"firms\", threshold=20000):\n",
    "    if large_by == \"population\" and \"cbsa_pop\" in xwalk_df.columns:\n",
    "        pop = xwalk_df.drop_duplicates(subset=[\"cbsa_code\",\"cbsa_title\"])[[\"cbsa_code\",\"cbsa_title\",\"cbsa_pop\"]]\n",
    "        cbsa_totals = (cbsa_df.drop_duplicates(subset=[\"cbsa_code\",\"cbsa_title\"])[[\"cbsa_code\",\"cbsa_title\"]]\n",
    "                            .merge(pop, on=[\"cbsa_code\",\"cbsa_title\"], how=\"left\"))\n",
    "        big_codes = cbsa_totals.loc[cbsa_totals[\"cbsa_pop\"] >= threshold, [\"cbsa_code\",\"cbsa_title\"]]\n",
    "    else:\n",
    "        totals = (cbsa_df.groupby([\"cbsa_code\",\"cbsa_title\"], as_index=False)[\"cbsa_tot_firms\"]\n",
    "                        .max())\n",
    "        big_codes = totals.loc[totals[\"cbsa_tot_firms\"] >= threshold, [\"cbsa_code\",\"cbsa_title\"]]\n",
    "    return cbsa_df.merge(big_codes, on=[\"cbsa_code\",\"cbsa_title\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7fdbd-bb20-4c64-9777-f0803e80bc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a14cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'abs_county_naics3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m abs_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mABS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m abs_df = normalize_abs_columns(abs_df)\n\u001b[32m      3\u001b[39m xwalk_df = pd.read_csv(XWALK_PATH, dtype=\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'abs_county_naics3.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "abs_df = pd.read_csv(ABS_PATH, dtype=str)\n",
    "abs_df = normalize_abs_columns(abs_df)\n",
    "xwalk_df = pd.read_csv(XWALK_PATH, dtype=str)\n",
    "xwalk_df = normalize_crosswalk(xwalk_df)\n",
    "\n",
    "print(\"ABS rows:\", len(abs_df), \"| Crosswalk rows:\", len(xwalk_df))\n",
    "abs_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recon = reconcile_county_totals(abs_df, year=YEAR, atol=RECON_ATOL)\n",
    "display_cols = [\"state\",\"county\",\"sum_firmpdemp\",\"tot_firmpdemp\",\"delta_firmpdemp\",\"pct_delta_firmpdemp\",\"recon_ok\"]\n",
    "recon_head = recon[display_cols].head(10)\n",
    "recon_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5283498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cbsa = aggregate_to_cbsa(abs_df, xwalk_df, year=YEAR)\n",
    "cbsa.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cbsa_large = filter_large_cbsa(cbsa, xwalk_df, large_by=LARGE_BY, threshold=LARGE_THRESHOLD)\n",
    "cbsa_large.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outdir = Path(OUTDIR); outdir.mkdir(parents=True, exist_ok=True)\n",
    "recon.to_csv(outdir / \"abs_county_naics3_recon_report.csv\", index=False)\n",
    "cbsa.to_csv(outdir / \"abs_cbsa_naics3.csv\", index=False)\n",
    "cbsa_large.to_csv(outdir / \"abs_cbsa_naics3_large.csv\", index=False)\n",
    "\n",
    "# Also: flag counties with reconciliation issues joined to CBSA for context\n",
    "cw_small = xwalk_df.rename(columns={\"state_fips\":\"state\", \"county_fips\":\"county\"})\n",
    "bad = recon[~recon[\"recon_ok\"]].merge(cw_small, on=[\"state\",\"county\"], how=\"left\")\n",
    "bad.to_csv(outdir / \"abs_cbsa_naics3_discrepancies.csv\", index=False)\n",
    "\n",
    "print(\"Wrote outputs to:\", outdir.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
