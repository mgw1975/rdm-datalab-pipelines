{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b707df-e07f-4920-b971-9c46264729da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import json\n",
    "from textwrap import dedent\n",
    "\n",
    "# 1) Peek at the user's backlog to surface \"row 4\" context (0-index vs 1-index can be ambiguous).\n",
    "backlog_path = \"/mnt/data/RDM_Backlog_With_ICE.csv\"\n",
    "try:\n",
    "    backlog_df = pd.read_csv(backlog_path)\n",
    "    # Create a helper column with 1-index row numbers for clarity.\n",
    "    backlog_df.insert(0, \"Row#\", range(1, len(backlog_df) + 1))\n",
    "    # Try to locate rows with ICE score 40 to help the user confirm.\n",
    "    ice_40 = backlog_df[backlog_df.astype(str).apply(lambda r: any(\"40\" == str(x).strip() for x in r), axis=1)]\n",
    "except Exception as e:\n",
    "    backlog_df = None\n",
    "    ice_40 = None\n",
    "    backlog_error = str(e)\n",
    "\n",
    "# 2) Write a ready-to-run Python module that:\n",
    "#    - Loads ABS county×NAICS3 data\n",
    "#    - Verifies county totals reconcile to NAICS 00 (All sectors)\n",
    "#    - Aggregates to CBSA via county→CBSA crosswalk\n",
    "#    - Filters to \"large\" CBSAs (by total firms or an optional population column if present)\n",
    "#    - Saves outputs to CSV\n",
    "script = dedent(\"\"\"\n",
    "    # rdm_abs_naics3_cbsa.py\n",
    "    # -------------------------------------------\n",
    "    # Extend ABS pull to 3-digit NAICS for CBSAs,\n",
    "    # verify county totals reconcile to NAICS 00,\n",
    "    # then aggregate to CBSA and filter to \"large\" CBSAs.\n",
    "    #\n",
    "    # INPUTS (CSV, UTF-8):\n",
    "    #   1) abs_county_naics3.csv  (county × NAICS3)\n",
    "    #        Required columns (case-insensitive accepted):\n",
    "    #          state, county, naics, NAICS2022 or naics3\n",
    "    #          FIRMPDEMP (firms), EMP, PAYANN (in $1,000s), RCPPDEMP (receipts in $1,000s)\n",
    "    #          NAICS 00 total rows must also be present for each county (All sectors)\n",
    "    #   2) cbsa_county_crosswalk.csv\n",
    "    #        Required columns:\n",
    "    #          state_fips (2), county_fips (3), cbsa_code, cbsa_title\n",
    "    #        Optional:\n",
    "    #          cbsa_pop (for 'large' definition). If missing, we rank by ABS firms instead.\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #   - abs_county_naics3_recon_report.csv  (county-level reconciliation vs NAICS 00)\n",
    "    #   - abs_cbsa_naics3.csv                 (CBSA × NAICS3 rolled up from counties)\n",
    "    #   - abs_cbsa_naics3_large.csv           (only 'large' CBSAs)\n",
    "    #   - abs_cbsa_naics3_discrepancies.csv   (CBSA counties with reconciliation issues)\n",
    "    #\n",
    "    # USAGE:\n",
    "    #   python rdm_abs_naics3_cbsa.py --abs /path/to/abs_county_naics3.csv \\\\\n",
    "    #                                 --xwalk /path/to/cbsa_county_crosswalk.csv \\\\\n",
    "    #                                 --year 2022 \\\\\n",
    "    #                                 --large_by firms --large_threshold 20000\n",
    "    #\n",
    "    import argparse\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    \n",
    "    def zfill_series(s, n):\n",
    "        return s.astype(str).str.extract(r\"(\\\\d+)\", expand=False).fillna(\"\").str.zfill(n)\n",
    "    \n",
    "    def normalize_abs_columns(df):\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        # Make standard names\n",
    "        rename_map = {}\n",
    "        for want in [\"state\",\"county\",\"naics\",\"naics2022\",\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"]:\n",
    "            for actual in df.columns:\n",
    "                if actual.lower() == want:\n",
    "                    rename_map[actual] = want.upper() if want in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"] else want\n",
    "        df = df.rename(columns=rename_map)\n",
    "        # Derive NAICS3 (first 3 digits of NAICS2022 or naics)\n",
    "        if \"NAICS2022\" in df.columns:\n",
    "            df[\"naics3\"] = df[\"NAICS2022\"].astype(str).str[:3]\n",
    "        elif \"naics\" in df.columns:\n",
    "            df[\"naics3\"] = df[\"naics\"].astype(str).str[:3]\n",
    "        else:\n",
    "            raise ValueError(\"ABS file must have NAICS2022 or naics column\")\n",
    "        # Standardize FIPS\n",
    "        df[\"state\"] = zfill_series(df[\"state\"], 2)\n",
    "        df[\"county\"] = zfill_series(df[\"county\"], 3)\n",
    "        # Convert $1,000s → $\n",
    "        for k in [\"PAYANN\", \"RCPPDEMP\"]:\n",
    "            if k in df.columns:\n",
    "                df[k] = pd.to_numeric(df[k], errors=\"coerce\") * 1000\n",
    "        # Nullable numerics\n",
    "        for c in [\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        return df\n",
    "    \n",
    "    def normalize_crosswalk(df):\n",
    "        need = [\"state_fips\",\"county_fips\",\"cbsa_code\",\"cbsa_title\"]\n",
    "        missing = [c for c in need if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Crosswalk missing columns: {missing}\")\n",
    "        df[\"state_fips\"]  = zfill_series(df[\"state_fips\"], 2)\n",
    "        df[\"county_fips\"] = zfill_series(df[\"county_fips\"], 3)\n",
    "        if \"cbsa_pop\" in df.columns:\n",
    "            df[\"cbsa_pop\"] = pd.to_numeric(df[\"cbsa_pop\"], errors=\"coerce\")\n",
    "        return df\n",
    "    \n",
    "    def reconcile_county_totals(abs_df, year=None, atol=1.0):\n",
    "        \\\"\\\"\\\"Check that sum over NAICS3 equals NAICS 00 total at county level.\n",
    "        Returns a report with absolute and percent deltas for EMP, PAYANN, RCPPDEMP, FIRMPDEMP.\n",
    "        atol in dollars and counts (post $1,000→$ conversion).\\\"\\\"\\\"\n",
    "        base = abs_df.copy()\n",
    "        if year and \"year\" in base.columns:\n",
    "            base = base[base[\"year\"] == year]\n",
    "        # Identify NAICS 00 \"All sectors\"\n",
    "        mask_all = base[\"naics3\"].isin([\"000\"]) | base.get(\"NAICS2022\",\"\").astype(str).isin([\"00\"]) | base.get(\"naics\",\"\").astype(str).isin([\"00\"])\n",
    "        all_rows = base[mask_all].copy()\n",
    "        # If NAICS 00 is encoded as \"00\" rather than \"000\", normalize it\n",
    "        all_rows[\"naics3\"] = \"000\"\n",
    "        parts = base[~mask_all].copy()\n",
    "        # Roll up by county\n",
    "        key = [\"state\",\"county\"]\n",
    "        sums = (parts.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                      .sum(min_count=1))\n",
    "        totals = (all_rows.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                          .sum(min_count=1))\n",
    "        rep = key.copy()\n",
    "        for c in [\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]:\n",
    "            sums[c] = pd.to_numeric(sums[c], errors=\"coerce\")\n",
    "            totals[c] = pd.to_numeric(totals[c], errors=\"coerce\")\n",
    "            sums = sums.rename(columns={c: f\"sum_{c.lower()}\"})\n",
    "            totals = totals.rename(columns={c: f\"tot_{c.lower()}\"})\n",
    "        report = sums.merge(totals, on=key, how=\"outer\")\n",
    "        for c in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"]:\n",
    "            report[f\"delta_{c}\"] = report[f\"sum_{c}\"] - report[f\"tot_{c}\"]\n",
    "            report[f\"pct_delta_{c}\"] = np.where(report[f\"tot_{c}\"].abs() > 0,\n",
    "                                                report[f\"delta_{c}\"] / report[f\"tot_{c}\"], np.nan)\n",
    "            report[f\"flag_{c}\"] = report[f\"delta_{c}\"].abs() > atol\n",
    "        # Any flag triggered?\n",
    "        report[\"recon_ok\"] = ~(report[[f\"flag_{c}\" for c in [\"firmpdemp\",\"emp\",\"payann\",\"rcppdemp\"]]].any(axis=1))\n",
    "        return report.sort_values(key)\n",
    "    \n",
    "    def aggregate_to_cbsa(abs_df, xwalk_df, year=None):\n",
    "        base = abs_df.copy()\n",
    "        if year and \"year\" in base.columns:\n",
    "            base = base[base[\"year\"] == year]\n",
    "        base[\"state_fips\"] = base[\"state\"]\n",
    "        base[\"county_fips\"] = base[\"county\"]\n",
    "        merged = base.merge(xwalk_df, on=[\"state_fips\",\"county_fips\"], how=\"left\", validate=\"m:1\")\n",
    "        # Roll up county→CBSA for NAICS3 (excluding NAICS 000 rows)\n",
    "        parts = merged[~merged[\"naics3\"].isin([\"000\"])].copy()\n",
    "        key = [\"cbsa_code\",\"cbsa_title\",\"naics3\"]\n",
    "        out = (parts.groupby(key, as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                    .sum(min_count=1))\n",
    "        # Also keep CBSA totals (All sectors) for reference\n",
    "        all_cbsa = (merged[merged[\"naics3\"].isin([\"000\"])]\n",
    "                    .groupby([\"cbsa_code\",\"cbsa_title\"], as_index=False)[[\"FIRMPDEMP\",\"EMP\",\"PAYANN\",\"RCPPDEMP\"]]\n",
    "                    .sum(min_count=1)\n",
    "                    .rename(columns={\n",
    "                        \"FIRMPDEMP\":\"cbsa_tot_firms\",\n",
    "                        \"EMP\":\"cbsa_tot_emp\",\n",
    "                        \"PAYANN\":\"cbsa_tot_payroll\",\n",
    "                        \"RCPPDEMP\":\"cbsa_tot_receipts\"\n",
    "                    }))\n",
    "        out = out.merge(all_cbsa, on=[\"cbsa_code\",\"cbsa_title\"], how=\"left\")\n",
    "        return out\n",
    "    \n",
    "    def filter_large_cbsa(cbsa_df, xwalk_df, large_by=\"firms\", threshold=20000):\n",
    "        # If population available, prefer it when large_by=='population'\n",
    "        if large_by == \"population\" and \"cbsa_pop\" in xwalk_df.columns:\n",
    "            pop = xwalk_df.drop_duplicates(subset=[\"cbsa_code\",\"cbsa_title\"])[[\"cbsa_code\",\"cbsa_title\",\"cbsa_pop\"]]\n",
    "            cbsa_totals = (cbsa_df.drop_duplicates(subset=[\"cbsa_code\",\"cbsa_title\"])\n",
    "                                [[\"cbsa_code\",\"cbsa_title\"]].merge(pop, on=[\"cbsa_code\",\"cbsa_title\"], how=\"left\"))\n",
    "            big_codes = cbsa_totals.loc[cbsa_totals[\"cbsa_pop\"] >= threshold, [\"cbsa_code\",\"cbsa_title\"]]\n",
    "        else:\n",
    "            # Fallback: size by CBSA total firms\n",
    "            totals = (cbsa_df.groupby([\"cbsa_code\",\"cbsa_title\"], as_index=False)[\"cbsa_tot_firms\"]\n",
    "                            .max())\n",
    "            big_codes = totals.loc[totals[\"cbsa_tot_firms\"] >= threshold, [\"cbsa_code\",\"cbsa_title\"]]\n",
    "        large = cbsa_df.merge(big_codes, on=[\"cbsa_code\",\"cbsa_title\"], how=\"inner\")\n",
    "        return large\n",
    "    \n",
    "    def main():\n",
    "        ap = argparse.ArgumentParser()\n",
    "        ap.add_argument(\"--abs\", required=True, help=\"Path to ABS county×NAICS3 CSV\")\n",
    "        ap.add_argument(\"--xwalk\", required=True, help=\"Path to county→CBSA crosswalk CSV\")\n",
    "        ap.add_argument(\"--year\", type=int, default=None, help=\"Optional year filter\")\n",
    "        ap.add_argument(\"--large_by\", choices=[\"firms\",\"population\"], default=\"firms\")\n",
    "        ap.add_argument(\"--large_threshold\", type=int, default=20000)\n",
    "        ap.add_argument(\"--recon_atol\", type=float, default=1.0, help=\"Absolute tolerance for reconciliation deltas\")\n",
    "        ap.add_argument(\"--outdir\", default=\".\", help=\"Directory to write outputs\")\n",
    "        args = ap.parse_args()\n",
    "    \n",
    "        abs_df = pd.read_csv(args.abs, dtype=str)\n",
    "        # Cast numerics later inside normalize\n",
    "        abs_df = normalize_abs_columns(abs_df)\n",
    "        xwalk_df = pd.read_csv(args.xwalk, dtype=str)\n",
    "        xwalk_df = normalize_crosswalk(xwalk_df)\n",
    "    \n",
    "        # 1) Reconciliation at county-level\n",
    "        rep = reconcile_county_totals(abs_df, year=args.year, atol=args.recon_atol)\n",
    "        outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)\n",
    "        rep_path = outdir / \"abs_county_naics3_recon_report.csv\"\n",
    "        rep.to_csv(rep_path, index=False)\n",
    "    \n",
    "        # 2) Aggregate to CBSA\n",
    "        cbsa = aggregate_to_cbsa(abs_df, xwalk_df, year=args.year)\n",
    "        cbsa_path = outdir / \"abs_cbsa_naics3.csv\"\n",
    "        cbsa.to_csv(cbsa_path, index=False)\n",
    "    \n",
    "        # 3) Filter to 'large' CBSAs\n",
    "        large = filter_large_cbsa(cbsa, xwalk_df, large_by=args.large_by, threshold=args.large_threshold)\n",
    "        large_path = outdir / \"abs_cbsa_naics3_large.csv\"\n",
    "        large.to_csv(large_path, index=False)\n",
    "    \n",
    "        # 4) Flag CBSA counties with reconciliation issues (join county report back to crosswalk)\n",
    "        #    Useful to understand whether any CBSA totals might be biased by county-level suppression.\n",
    "        rep_key = [\"state\",\"county\"]\n",
    "        cw_small = xwalk_df.rename(columns={\"state_fips\":\"state\",\"county_fips\":\"county\"})\n",
    "        bad = rep[~rep[\"recon_ok\"]].merge(cw_small, on=rep_key, how=\"left\")\n",
    "        bad_path = outdir / \"abs_cbsa_naics3_discrepancies.csv\"\n",
    "        bad.to_csv(bad_path, index=False)\n",
    "    \n",
    "        print(f\"Wrote: {rep_path}\")\n",
    "        print(f\"Wrote: {cbsa_path}\")\n",
    "        print(f\"Wrote: {large_path}\")\n",
    "        print(f\"Wrote: {bad_path}\")\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "\"\"\")\n",
    "\n",
    "script_path = \"/mnt/data/rdm_abs_naics3_cbsa.py\"\n",
    "with open(script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(script)\n",
    "\n",
    "# Save a lightweight README with instructions and expected schemas.\n",
    "readme = dedent(\"\"\"\n",
    "    RDM ABS NAICS3 → CBSA Pipeline\n",
    "    ===============================\n",
    "    \n",
    "    Files you need\n",
    "    --------------\n",
    "    1) `abs_county_naics3.csv` — ABS data at **county × NAICS3** with NAICS 00 totals.\n",
    "       - Required columns (case-insensitive accepted):\n",
    "         state, county, naics or NAICS2022, FIRMPDEMP, EMP, PAYANN, RCPPDEMP, (optional) year\n",
    "       - NOTE: PAYANN and RCPPDEMP must be in $1,000s (as published) — the script converts to $.\n",
    "    \n",
    "    2) `cbsa_county_crosswalk.csv` — County → CBSA mapping.\n",
    "       - Required: state_fips (2), county_fips (3), cbsa_code, cbsa_title\n",
    "       - Optional: cbsa_pop (to size “large” CBSAs by population). If absent, we size by ABS firms.\n",
    "    \n",
    "    What the script does\n",
    "    --------------------\n",
    "    • Reconciles county sums across NAICS3 to the NAICS 00 (All sectors) total.\n",
    "    • Aggregates counties to CBSA for NAICS3 rows (excludes NAICS “000”). Keeps CBSA totals for reference.\n",
    "    • Filters to “large” CBSAs using either population or total ABS firms.\n",
    "    • Writes four CSVs: reconciliation report, CBSA NAICS3, CBSA NAICS3 (large only), and a list of counties with reconciliation flags.\n",
    "    \n",
    "    Quick start\n",
    "    -----------\n",
    "    python rdm_abs_naics3_cbsa.py --abs abs_county_naics3.csv \\\\\n",
    "                                  --xwalk cbsa_county_crosswalk.csv \\\\\n",
    "                                  --year 2022 \\\\\n",
    "                                  --large_by firms --large_threshold 20000 \\\\\n",
    "                                  --outdir outputs\n",
    "    \n",
    "    Tip\n",
    "    ---\n",
    "    If your NAICS 00 is coded as \"00\" instead of \"000\", the script normalizes that internally.\n",
    "    Expect some reconciliation flags where ABS suppression/rounding occurs.\n",
    "\"\"\")\n",
    "readme_path = \"/mnt/data/README_ABS_NAICS3_CBSA.txt\"\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "# Prepare a tiny crosswalk template to guide the user.\n",
    "crosswalk_template = pd.DataFrame({\n",
    "    \"state_fips\": [\"06\",\"06\"],\n",
    "    \"county_fips\": [\"001\",\"087\"],\n",
    "    \"cbsa_code\": [\"41860\",\"41940\"],\n",
    "    \"cbsa_title\": [\"San Francisco-Oakland-Berkeley, CA\",\"San Jose-Sunnyvale-Santa Clara, CA\"],\n",
    "    \"cbsa_pop\": [4731000, 1996000]\n",
    "})\n",
    "crosswalk_path = \"/mnt/data/cbsa_county_crosswalk_template.csv\"\n",
    "crosswalk_template.to_csv(crosswalk_path, index=False)\n",
    "\n",
    "# Display helpful tables (if backlog was readable)\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "if backlog_df is not None:\n",
    "    display_dataframe_to_user(\"RDM Backlog (with row numbers)\", backlog_df)\n",
    "    if ice_40 is not None and len(ice_40) > 0:\n",
    "        display_dataframe_to_user(\"Backlog rows containing ICE score '40' (string match)\", ice_40)\n",
    "\n",
    "{\n",
    "    \"script_path\": script_path,\n",
    "    \"readme_path\": readme_path,\n",
    "    \"crosswalk_template_path\": crosswalk_path,\n",
    "    \"backlog_loaded\": backlog_df is not None\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
